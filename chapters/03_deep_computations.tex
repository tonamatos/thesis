\chapter{Deep Computations}\label{ch:deep_computations}

This is the deep computations chapter of the thesis.

This work was presented over the course of multiple sessions at the Fields Institute.

\section{Introduction}\label{sec:deep_comp_intro}

\section{Connections to learning theory}\label{sec:learning_theory}

I formalize the connection between deep computations and learning theory.
Here I include a series of examples connecting different variantes of the NIP.
Expanding on the initial concepts from \cite{Duenezetal2026}, I explicitly map the relationships between the Non-Independence Property (NIP), VC-dimension, and PAC-learnability within this new context.

\section{The Bourgain-Fremlin-Talagrand theorems}\label{sec:bft}

I synthesize the Bourgain-Fremlin-Talagrand theorems, organizing and refining the material presented during four seminars at the Fields Institute into a cohesive reference for this field.
The first parts of the theory were presented on the date\ref{} and then also on this other day.

\section{The non-independence property}\label{sec:non_independence}

I analyze the model-theoretic aspects of deep computations, isolating the specific role NIP plays in the combinatorial properties of definable sets.

\section{Compositional computation structures}\label{sec:examples_ccs}

This section details my primary contribution to \cite{Duenezetal2026}: the construction of concrete examples of computation structures that separate learnability classes.
These examples provide the necessary counterpoints to establish the boundaries of the theory.

\section{Quantum logics}\label{sec:quantum_logics}

Work in progress...